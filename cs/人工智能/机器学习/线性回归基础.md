Title: 线性回归基础  
Date: 2016-7-16
slug: linear-regression-foundation  
category: 机器学习
tags: CS, AI, Machine Learning
Modified: 2016-07-16

[TOC]

# 说明

本文参考以下文献：

1. Andrew Ng在斯坦福的cs229讲义， [cs229](http://cs229.stanford.edu/materials.html)

在下文笔较烂， 恐贻笑大方。 不过因为是markdown 写的， 有不足， 改进比较方便， 所以欢迎提出意见及建议，找出问题， 谢谢。

# 导读

关于机器学习的概念、定义，我写不明白。 监督学习、无监督学习的区别， 分类问题和回归问题的区别也写不好，所以这里假设读者对这些概念都有一定了解。

本节要讲的内容就是线性回归及其求解。

一言不合就问问题。

{%radio|同学你知道什么是线性回归吗？&知道&不知道@不知道#我个人水平问题，请选不知道%}

## 是什么？

线性回归是什么？

线性简单，代表模型、理论函数是 y=w*x+b, 这里的x可以是一个变量，也可以是向量。

回归是什么？建议找本统计学的书来看看， 比如陈希孺《概率论与数理统计》第258页开始， 介绍回归分析的基本概念。

我们假设了模型，但模型的参数未知， 回归分析就是去分析、求出参数的值， 并达到某个效果。

和分类问题相比， 分类是输出“类别”，离散的值， 回归输出的是连续的值。

## 为什么？

为什么是线性回归， 首先线性肯定比非线性简单， 其次跟线性代数、 统计课程能连上。 但为什么线性模型足够有效，超过我的知识水平了。


## 总览

机器学习有三要素（李航《统计学习方法》统计学习方法的三要素）：

1. 模型
2. 策略
3. 算法

所以，本文内容相应的就是：

1. 模型--线性模型， y=w*x+b
2. 策略--确定模型（参数）选择的准则（评价方法），即损失\误差函数。用策略一词，是有点不习惯。
3. 算法--求解最优模型的过程。

# 模型

前面说过多回了， 线性回归， 线性方程 y=h(x)=$\sum \limits_{i=0} w_i*x_i$, 这里把b看作是 $w_0 * x_0$

这里采用这种标记法，另有一种常用的是$Y=\theta^TX$。只是符号问题。

接下来， 我们就要考虑求最优模型（参数）。

# 策略

为了求解最优模型， 需要采取一定的策略。机器学习里的策略大概就是指模型选择的准则。

对线性回归来说，我们考虑让预测值和实际值之间误差最小作为标准，选取损失函数。 篇幅限制及原课程的安排， 所以， 推导具体的损失函数请看 [概率解释]()

{%radio|是否已看过概率解释部分？&看了&没有@看了#建议先看看%}

## 损失函数

上结论， 我们选择评估误差的标准为 $(h(x) - y)^2$, h(x)=w*x是预测值，y是实际值。平方强调了误差越大，影响越大。

这是单个数据点的误差， 所以整个数据集的误差就是：
$$
J(w)=\sum^m_{i=1}(h(x^i) - y^i)^2
$$

于是， 任务就变成了对上式求最小化（误差最小），并求出最小是h(x)的参数w。

$$
\min\limits_w\sum^m_{i=1}(h(x^i) - y^i)^2
$$

# 算法

如果了解、熟悉梯度下降法的同学已经可以不用看了。直接点下一段到结束吧！

$x^2$平方函数有什么特点呢？一个开口向上的抛物线， 对吧？所以它确实存在最小值。而损失函数也会存在最小值， 这个我也不会证，但应该没有疑义吧？

在训练集数据x,y都已知的情况， 损失函数的自变量是否只有一个w了？损失函数的值会随着w的变化而变化。

我们假设一个一维w对应的损失函数如图：

![找最小](http://7xt8es.com1.z0.glb.clouddn.com/zhimind/ml/findMinimum.png)

人眼能估计最小值在哪个位置对吧， 先说明下，我写的函数是 $y=3*x^2-2*x+5$。所以最小值不是在x=0。

{%radio|你觉得应该怎么做?&不知道&从非常小到非常大全部试&从一个随机位置开始&公式变换找最小位置@从一个随机位置开始#呃%}

## 从随机状态开始

暴力搜索地从非常小到非常大地尝试肯定是不可取的，又慢又试不完。PASS。

公式变换对于已知系数一元二次方程是可行的， 但现在就是要去求出系数。PASS。

只能从某个随机值开始，以前学的算法多数是确定性算法， 貌似只有快速排序里可选地提及随机选择。

而现实中， 类似的搜索性问题多半是从随机初始状态开始的。这可能算是个很重要的**随机化**思路。

我们现在随机选择了x=5这一点（实际可能喜欢用0向量）， 在上图的红点有个X。这里是个坡，我们放个小球，会自然地顺着坡滚下去。那计算机程序怎么办呢？一维的w在图上还只有左右两个方向，二维的w加上y值形成3D图，每个点都是360度连续空间

![3d找最小](http://7xt8es.com1.z0.glb.clouddn.com/zhimind/ml/findMinimum-3d.png)

怎么选？

{%radio|&随机选&采样求平均&求坡度&求最小&不知道&求反函数@求坡度#顺着坡滚%}

## 坡度和梯度

坡度是什么呢？

{%radio|&极值&导数&反函数&指数&对数&二阶导数@导数#这个，那个%}

## 导数

很明显了， 我们要求导！！！  一维的w，也就是只有一个变量， 求导肯定对的，非左即右。 

那2维、多维的w呢？拿2维来说，就相当于x轴和y轴，要同时在两个变量上求导~~~所以， 分别对两个变量求偏导。并可以类推到更高维。

高数、线代没学好，这里强词夺理、囫囵吞枣。

## 求偏导

终于， 绕过来了， 需要要对J（w）函数求偏导，找出当前点的“滚动”方向。写作$
\frac{\partial}{\partial w}J(w)$。

其中,忽略求和符号后，$J(w)=(h(x) - y)^2$。一个简单的复合函数。

本以为，终于可以有填空题了。结果发现失败了。

{%text|复合函数求导h(x)=f(g(x)),则h'(x)=_?不需要*符号@f'(g(x))g'(x)#这个应该OK%}



## 继续求导

复合函数的求导法则知道了， 来推导本例子。

$$
\begin{aligned} 
\frac{\partial}{\partial w_i}J(w)
& = \frac{\partial}{\partial w}(h(x) - y)^2 \\
& = 2*(h(x)-y) * \frac{\partial}{\partial w_i}(h(x) - y) \\
\end{aligned}
$$

想实现读者自行推导过程， 辅助提醒， 但公式判断怎么做？搜都不知道搜什么。强行中断一下。

已知 h(x) = w*x, 等于向量w和x的内积

那 $\frac{\partial}{\partial w_i}(h(x) - y)$化简结果是？ 

{%radio|&w向量的i分量&x的i分量&x的和&w的和@x的i分量#w的i分量被微分了#w的非i分量直接当常数没了%}

## 求导结果

所以结果为：

$$
\frac{\partial}{\partial w_i}J(w) = 2*(h(x)-y)*x_i 
$$

求偏导给出了方向， 所以， 要沿着方向。那么，

{%radio|把w看作点的位置，怎么滚？&w-偏导值&偏导值替换w&二者相乘&二者相加@w-偏导值#偏导值为正，代表什么？%}

## 滚多远？

所以 w是当前坐标， 要减掉偏导得到的结果（沿着梯度方向滚）， 才能使得J(w)变小。

我们能沿着这个方向没没完没了地滚下去吗？ 要知道哪怕只滚动一厘米、 一毫米， 新的位置的偏导值（向量）都有可能发生变化。

{%radio|所以要怎么样？&给偏导值向量乘上个小系数&无聊&乘上个大系数&偏导值改成倒数@给偏导值向量乘上个小系数#都提示不要变化太大了%}

## 什么时候结束

$\alpha$的取值太大太小都有问题，这是工程实践上的问题，跳过。

综上所述， 我们有 $w=w-\alpha \frac{\partial}{\partial w}J(w)$， 作为每次迭代对w的更新。

迭代总要有个停。大概可能有以下这些方案：

1. 迭代指定次数
2. 两次迭代的J(w)值之差足够小——这个好像比较常用。
3. 偏导值奇迹般地等于0

## 算法部分总结

以上就是线性回归求解的算法，即梯度下降法

大致步骤为：

1. 已知一个待求最优（最小或最大）的一元、多元函数
2. 给自变量选取一个随机的起始值
3. 对自变量的各个分量求偏导
4. 根据偏导的方向（值）来适当更新自变量
5. 迭代3、4步直到满足你设定的收敛或其他条件

# 梯度下降实践

梯度下降法在更新w上可以采取不同的策略。 从之前的公式， 我们得到完整用于整个训练集的公式为（原先的2是常数，跟alpha合并即可）：

$$
w_i = w_i - \alpha \sum^m_{j=1}(h(x^j)-y^j)*x^j_i 
$$

这个式子每次都要把整个训练集X求个和， 所以叫**批量梯度下降** Batch Gradient Descent

在数据量比较大的时候就会很慢。

那相应的就是不批量策略。比如stochastic随机梯度下降也叫增量梯度下降。

简单来说， 就是不求和，不停扫描。

Repeat {
   for j=1 to m {    
$
   w_i = w_i - alpha * (h(x^j) - y^j) * x^j_i 
$  (for all i)   
   }
}

# 恭喜

线性回归的主要部分就是这些， 谢谢您的参与。
