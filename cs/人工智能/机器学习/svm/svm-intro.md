Title: 支持向量机系列一之介绍   
Date: 2016-04-19
slug: svm-1-max-margin
category: 机器学习   
tags: CS, 机器学习, 人工智能  
Modified: 2016-04-19
summary: 支持向量机最大间隔介绍

[TOC]

## 免责说明

本文参考以下文献：

1. pluskid支持向量机系列博文，特别是图片引用 [pluskid-svm](http://blog.pluskid.org/?page_id=683)
2. Andrew Ng在斯坦福的cs229讲义， [cs229](http://cs229.stanford.edu/materials.html)

在下文笔较烂， 恐贻笑大方。 不过因为是markdown 写的， 有不足， 改进比较方便， 所以欢迎提出意见及建议，找出问题， 谢谢。

## 导读

支持向量机support vector machine(SVM)有着很长很长的故事， 然而我并不熟悉~~~略过

默认读者已经了解机器学习领域里的分类 和 回归 的概念， 若不懂， 请参考 [敬请期待]() 和 [请看下回]()

先下结论， SVM是效果最好、 现成可用的分类算法之一， 虽然也有被用来做回归的。

所以， 让我们从二元线性分类问题开始， 慢慢引出SVM。

## 分类器

### 分类问题定义

默认读者已经了解 分类问题， 但定义还是要写写

定义：

数据集 $X = [x_1,...,x_m], 其中 x_i$ 一般是一个n维列向量， 代表一个数据点，写程序时貌似更喜欢一行代表一个点

对应类别 $y$, 用 1，-1 或 0，1 代表不同的分类。

分类问题就是要在 n维的数据空间中找出一个超平面, 希望通过这个超平面可以把两类数据完全分隔开来， 设函数 $f(x)$, 该超平面方程为 f(x) = 0, 那么定义：

1. 超平面上的点都有 f(x) = 0
2. 对 f(x) < 0的点， 就令 y = -1
3. 对 f(x) > 0的点， 就令 y = 1

用于提神的第一个问题(没水平， 连问题都编不出来), 



### 线性分类器

我们先从简单的线性分类器开始。 先来个前提， 假设数据确实线性可分。

假设该超平面的权重系数为 n维向量 w， bias偏置项为 b， x为n维列向量的点。

用于醒脑的第二个问题


## 分类结果-超平面

所以超平面方程可表示为： $w^Tx + b = 0， 即 f(x) = w^Tx + b$

我们知道， 超平面在二维空间中的例子就是一条直线, 如图所示(直接从pluskid博客上引用过来,需要自己另行制作吗？)：

![pluskid-hyperplane](http://blog.pluskid.org/wp-content/uploads/2010/09/Hyper-Plane.png)

### 多个超平面

以上图为例， 我们知道，除了已有的这条啥颜色直线， 还有无数条直线， 都可以把红蓝点正确地分成两块。

比如， 用逻辑回归， 不同的初始值就可能得到不同的直线。

既然有无数种可能， 那我们应该如何选择？我承认问题和选项有点别扭~~~还好不是关键步骤。

{%radio|&随机选一条&多条取平均&放弃&找出最好的一个@找出最好的一个#感觉有点别扭的问题~~~#答案是找最好的%}

### 评估方法

既然有无数种可能， 那么只要条件允许（时间、 金钱等）， 我们就要精益求精， 做到最好！ 完美！

言归正传， 哪个超平面是最好的呢？ 那就是个新的问题——怎么评估， 评估标准是什么。

你觉得评估标准应该是什么？

{%checkbox|&w范数值最小&超平面到所有点的距离之和最大&超平面到最近点的距离最大&w范数最大@超平面到最近点的距离最大#虽然是多选题，svm只有一个选项(我的理解)%}

## 评估超平面-距离

范数没用， theta 乘上个常数， 范数就变了， 但超平面不变——不进行数学推导了。

所有点距离之和 与 最近点距离 有点像， 但可能就是求所有点的距离 计算量可能大了点， 或者 数学推导出来效果不好

所以 只要找出 到相应最近点距离最大的超平面就好。（或许以后有人能找出其他方法~~~）

就是缓冲地带最大

### 定义

那我们说距离、 或者间隔最大， 这里距离是指什么？

{%radio|&f(x)的绝对值&点到超平面的垂线距离&不知道&编不出来选项了@点到超平面的垂线距离#超平面到相应最近点距离，第一反应不可能是其他吧%}

### 几何距离

所以， 点到超平面的垂线距离 定义为 几何距离。

关于 $f(x) = w^Tx+b=0$的超平面P 和 点z 的几何距离的计算或者说通用式， 建议不熟悉、 没印象的读者动手推导一下。

推导过程。 见[后续补充]()

先上结论： 点到超平面的几何距离(可能有误， 所以说要动手推导一下)

$$ 
\gamma = \frac{w^Tx+b}{\|w\|}=\frac{f(x)}{\|w\|}
$$

本式有个问题， 不能做为距离使用， 请指出

{%radio|&||w||未定义&||w||是范数，范数有很多种&结果可能为负&跟类别y无关&太简单&推导太复杂&不知道@结果可能为负#距离一定是什么样的？%}

### 标准定义

上式存在负值的可能， 距离是不可能用负值的， 所以再取个绝对值， 等效于乘以 y, 因为 f(x)小于0时, y = -1. 仍使用 gamma 记号
$$
几何距离: \gamma = |\gamma| = y\gamma = y*\frac{w^Tx+b}{\|w\|}=\frac{y*f(x)}{\|w\|}
$$

所以， 单点到超平面距离知道怎么求了， 下一步呢？

{%radio|&随机找一点的几何距离&求全部点的距离之和&找最远点&找最近点&求平均值&求平均值后找最接近平均值的一个点@找最近点#还记得如何评估超平面吗？%}

## 几何间隔

已知某个点到超平面的几何距离（几何间隔）

$$ \gamma = y*\frac{w^Tx+b}{\|w\|}=\frac{y*f(x)}{\|w\|} $$

我们知道， 要找的是超平面的最近点。

### 定义

最近点到其超平面的几何距离， 就是所有点到该超平面的几何距离的最小值， 定义为该平面的几何间隔geometrical margin （当然我这个概念理解 可能有问题， 但大体上不影响理解）

所以有 几何间隔:

$$
\tilde{\gamma} = \min_{i=1,...,m} \gamma^{(i)}
$$

### 函数间隔

一般的教程 会同时介绍个 函数间隔  functional margin。 但咱们秉承从直观分析开始， 需要时再合理推导的精神， 先不讨论这个玩意(貌似后面就没讨论过， 真可以不要）， 虽然二者等价， 但我们现在只要一个就够。

直接上定义（单点 函数距离 求最小为间隔， 公式里写的是函数距离， 省略求最小）

$$\hat{\gamma}=y(w^Tx+b)=yf(x)$$

已知几何间隔，

{%radio|那下一步呢?&不知道&梯度下降法&求最大化&蒙特卡罗模拟&酱油@求最大化#还记得要做什么吗？%}

## 最大间隔

还记得我们要做什么吧？ 找出到相应最近点距离最大的超平面， 也就是几何间隔最大的 超平面。

如下：

$$
\begin{align}
&\max_{w,b} \tilde{\gamma} \\
& 
\begin{array}
&s.t.
&y^{(i)}(w^Tx^{(i)} + b) ≥ \tilde{\gamma},  &i=1,\ldots,m\\
\end{array}
\end{align}
$$

即：

$$
\begin{align}
&\max_{w,b} \min_{i=1,...,m}  \frac{y^{(i)}(w^Tx^{(i)}+b)}{\|w\|} 
\end{align}
$$

为美观着想， 忽略求最近点，即求最小的min符号及i标号

$$
\begin{align}
&\max_{w,b} \frac{y(w^Tx+b)}{\|w\|} \\
& 
\begin{array}
&s.t.
&y^{(i)}(w^Tx^{(i)} + b) ≥ \tilde{\gamma}*||w||,  &i=1,\ldots,m\\
\end{array}
\end{align}
$$

### 一帆风顺地旅途到此结束

到上面的公式（latex给公式上标号 不会啊）， 应该是已经得到一个待优化的问题。

那我们还能再进一步优化吗？我们要真正地去发现、 分析、 探索。做科研就是这样， 但因为没有具体的方向， 我们就要有一些指导性的原则， 遇到 什么问题， 大概用什么样的思路地尝试。

技术问题， 暂不提供无标准答案的填空题， 有心的读者请先再草稿纸上写写你的想法， 你要来优化， 会怎么做？写思路。

## 优化

所以， 就来看这个式子

$$ \max_{w,b} \frac{y(w^Tx+b)}{\|w\|}=\frac{\hat{\gamma}}{||w||} $$

首先， $w 和 ||w||$ 和 $b$ 是未知的， 待求的， 肯定没法下手了。

那上面的这个 $\hat{\gamma}$ 是不是能做点文章呢? 

y 就是给它取正，取正 = 取绝对值， 平方也能取正， 好， 小本子上记下， 来个求平方？

还有呢？ $w^Tx+b$ 是个线性关系， 对吧？

所以， 它有什么性质， 而且还是个跟除法有关的性质呢？

{%radio|&不知道&编不出来&等比变化不变&最简方程&可线性组合@等比变化不变#答案比较明显，选项太难编了%}

### 等比变化

那小本本上的求平方就被我们扔脑后了， 求平方还会搞得更复杂。

将$\hat{\gamma}=y(w^Tx+b) 的 w和b$按比例变化时， 有两点

1. 对目标函数的优化没影响， 这个比较直观， 因为方程变化后， 最近点还是最近点， 几何间隔（最近点到超平面的几何距离 ）$\tilde{\gamma}$不变。
2. 不等式约束不变化， 没影响。 几何间隔$\tilde{\gamma}$不变, 左式变化的比例， 会等比地影响到$||w||$。

那我们就开始搞坏事了~~~就要去给它变一下。 怎么变？

{%radio|&等于0&等于1&除以几何间隔&除以||w||@除以几何间隔#效果相同，但是原因更重要（仅仅个人观点），而且貌似也不对%}

#### 别人的解释

好吧， 我还是无法严密地说清楚这一变化的原因。

李航的《统计学习方法》就说 $\hat{\gamma}$的取值不影响最优化问题的解

pluskid 写的是固定变量， 固定的方式有两种：

1. 固定 $\|w\|$ 
2. 固定 $\hat{\gamma}$ ，

出于方便推导和优化的目的，选择第二种

Ng的描述为：

Recall our earlier discussion that we can add an arbitrary scaling constraint on w and b without changing anything. This is the key idea we’ll use now. 

We will introduce the scaling constraint that the functional margin of w, b with respect to the training set must be 1:
$$\hat{\gamma} = 1.$$

Since multiplying w and b by some constant results in the functional margin being multiplied by that same constant, this is indeed a scaling constraint, and can be satisfied by rescaling w, b. Plugging this into our problem above, and noting that maximizing
$$\hat{\gamma}/||w|| = 1/||w||$$

### 继续
总之， 令 $\hat{\gamma}=1$

则我们的目标函数化为： 
$$ 
\begin{align} 
&\max_{w,b} \frac{1}{\|w\|} \\
& 
\begin{array}
&
s.t., y_i(w^Tx_i+b)\geq 1, i=1,\ldots,m\\ \end{array}
\end{align}
$$ 

通过求解这个问题，我们就可以找到一个 margin 最大的 classifier ，如下图所示，中间的红色线条是 Optimal Hyper Plane ，另外两条线到红线的距离都是等于 $\tilde{\gamma}$ 的：

![optimal-hplane](http://blog.pluskid.org/wp-content/uploads/2010/09/Optimal-Hyper-Plane.png)

### 倒数变形

到上一步时， 我们得到了一个优化问题， 优化一个 `1/||w||`, 一个倒数， 不好。

根据最优化理论——我也不懂， $\max \frac{1}{||w||}$等价于$\min ||w||$

{%radio|有什么问题吗?&没问题&不成立&不能求导&不知道&值不连续@不能求导#最优化问题都少不了用偏导来梯度下降%}

### 等效变形

不能用一次的， 那就改成平方， 对求最优化问题没影响， 另外求导的原因， 再加上一个参数， 最终结果如下：

$$ 
\begin{align} 
&\min_{w,b} \frac{1}{2}\|w\|^2 \\
& 
\begin{array}
&
s.t., y_i(w^Tx_i+b)\geq 1, i=1,\ldots,m\\ \end{array}
\end{align}
$$ 

最终我们得到了一个凸优化问题，或者更具体地说，它是一个二次优化问题——目标函数是二次的，约束条件是线性的。这个问题可以用任何现成的 QP (Quadratic Programming) 的优化包进行求解。

## 总结 

### 复习
回忆一下， 我们整个过程做了哪些事。

1. 从二元线性分类开始， 我们明确要求超平面， 分类器都是这么干的。
2. 超平面有很多个， 我们就要有对超平面的评估标准。
3. 评估标准就要计算距离， 我们就用上了几何距离。
4. 有了几何距离， 我们就有最近点的几何距离， 就是几何间隔。
5. 知道了几何间隔， 我们就要求出最大的几何间隔 及 对应的超平面， 绕了一圈， 又绕回到第1条。
6. 优化公式里的 $\hat{\gamma}=y(w^Tx+b)$ 有着特殊的性质， 取值对最优化问题的解不影响。 所以我们把它设为1了。
7. 最后再把倒数形式的$\max \frac{1}{||w||}$ 转成 $\min \frac{1}{2}\|w\|^2$

### 恭喜！！！
一路艰辛， SVM 这才完成了最简单的部分， 后面理论 的推导更加复杂， 做好心理准备吧！

下一节[支持向量 并没有写出来]()

如果您觉得这种学习方式有帮助的话， 呃， 那就好~~~
